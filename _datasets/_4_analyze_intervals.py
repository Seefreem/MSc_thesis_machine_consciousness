"""Generated by GPT-5.1, reviewed and modified by Shiling Deng, 26/11/2025"""
import os
import json
from collections import Counter

from transformers import AutoTokenizer


DATA_DIR = "filtered_data"


def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_json(obj, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def compute_token_lengths(examples, text_key, tokenizer):
    """
    examples: list of dicts (each one example)
    text_key: key in example dict that contains the text (e.g. "text" or "sms")
    returns: list of ints (token lengths)
    """
    lengths = []
    for ex in examples:
        text = ex[text_key]
        encoded = tokenizer(
            text,
            add_special_tokens=True,
            padding=False,
            truncation=False,
        )
        lengths.append(len(encoded["input_ids"]))
    return lengths


def overlapping_interval(lengths_a, lengths_b):
    """
    lengths_a, lengths_b: lists of ints
    returns (low, high) overlap between [min_a, max_a] and [min_b, max_b]
    If there is no overlap, returns None.
    """
    min_a, max_a = min(lengths_a), max(lengths_a)
    min_b, max_b = min(lengths_b), max(lengths_b)

    low = max(min_a, min_b)
    high = min(max_a, max_b)

    if low > high:
        return None
    return low, high


def label_stats_for_interval(examples, lengths, interval, label_key="label"):
    """
    examples: list of dicts
    lengths: list of token lengths (same order as examples)
    interval: (low, high), inclusive
    returns: (selected_examples, Counter_of_labels)
    """
    low, high = interval
    selected_examples = []
    label_counter = Counter()

    for ex, L in zip(examples, lengths):
        if low <= L <= high:
            selected_examples.append(ex)
            label_counter[ex[label_key]] += 1

    return selected_examples, label_counter


def main():
    os.makedirs(DATA_DIR, exist_ok=True)

    # --- 1. Load filtered JSON files ---
    imdb_train_path = os.path.join(DATA_DIR, "stanfordnlp_imdb_train.json")
    imdb_test_path = os.path.join(DATA_DIR, "stanfordnlp_imdb_test.json")
    sms_train_path = os.path.join(DATA_DIR, "ucirvine_sms_spam_train.json")

    imdb_train = load_json(imdb_train_path)
    imdb_test = load_json(imdb_test_path)
    sms_train = load_json(sms_train_path)

    # --- 2. Load tokenizer ---
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")

    # --- 3. Compute token lengths for each split and save them ---
    imdb_train_lengths = compute_token_lengths(imdb_train, "text", tokenizer)
    imdb_test_lengths = compute_token_lengths(imdb_test, "text", tokenizer)
    sms_train_lengths = compute_token_lengths(sms_train, "sms", tokenizer)

    save_json(imdb_train_lengths, os.path.join(DATA_DIR, "stanfordnlp_imdb_train_token_lengths.json"))
    save_json(imdb_test_lengths, os.path.join(DATA_DIR, "stanfordnlp_imdb_test_token_lengths.json"))
    save_json(sms_train_lengths, os.path.join(DATA_DIR, "ucirvine_sms_spam_train_token_lengths.json"))

    print("Saved token length lists for all splits.")

    # --- 4. Compute overlapping intervals ---

    # int_1: overlap between imdb_train and sms_train
    int_1 = overlapping_interval(imdb_train_lengths, sms_train_lengths)
    if int_1 is None:
        print("\nNo overlapping interval (int_1) between imdb_train and sms_train.")
    else:
        print(f"\nint_1 (overlap imdb_train & sms_train) = [{int_1[0]}, {int_1[1]}]")

    # int_2: overlap between imdb_test and sms_train
    int_2 = overlapping_interval(imdb_test_lengths, sms_train_lengths)
    if int_2 is None:
        print("\nNo overlapping interval (int_2) between imdb_test and sms_train.")
    else:
        print(f"\nint_2 (overlap imdb_test & sms_train) = [{int_2[0]}, {int_2[1]}]")

    # --- 5. For int_1: select samples and print label statistics ---
    if int_1 is not None:
        print("\n=== Interval int_1 statistics (IMDB train & SMS train) ===")
        imdb_train_selected, imdb_train_label_stats = label_stats_for_interval(
            imdb_train, imdb_train_lengths, int_1, label_key="label"
        )
        sms_train_selected_1, sms_train_label_stats_1 = label_stats_for_interval(
            sms_train, sms_train_lengths, int_1, label_key="label"
        )

        print(f"IMDB train: selected {len(imdb_train_selected)} samples in int_1.")
        print(f"IMDB train label stats (label -> count): {dict(imdb_train_label_stats)}")

        print(f"SMS train: selected {len(sms_train_selected_1)} samples in int_1.")
        print(f"SMS train label stats (label -> count): {dict(sms_train_label_stats_1)}")

        save_json(imdb_train_selected, os.path.join(DATA_DIR, "stanfordnlp_imdb_train_samples_length_interval_1.json"))
        save_json(sms_train_selected_1, os.path.join(DATA_DIR, "ucirvine_sms_spam_train_samples_length_interval_1.json"))


    # --- 6. For int_2: select samples and print label statistics ---
    if int_2 is not None:
        print("\n=== Interval int_2 statistics (IMDB test & SMS train) ===")
        imdb_test_selected, imdb_test_label_stats = label_stats_for_interval(
            imdb_test, imdb_test_lengths, int_2, label_key="label"
        )
        sms_train_selected_2, sms_train_label_stats_2 = label_stats_for_interval(
            sms_train, sms_train_lengths, int_2, label_key="label"
        )

        print(f"IMDB test: selected {len(imdb_test_selected)} samples in int_2.")
        print(f"IMDB test label stats (label -> count): {dict(imdb_test_label_stats)}")

        print(f"SMS train: selected {len(sms_train_selected_2)} samples in int_2.")
        print(f"SMS train label stats (label -> count): {dict(sms_train_label_stats_2)}")

        save_json(imdb_test_selected, os.path.join(DATA_DIR, "stanfordnlp_imdb_test_samples_length_interval_2.json"))
        save_json(sms_train_selected_2, os.path.join(DATA_DIR, "ucirvine_sms_spam_train_samples_length_interval_2.json"))

if __name__ == "__main__":
    main()



# Saved token length lists for all splits.

# int_1 (overlap imdb_train & sms_train) = [12, 207]

# int_2 (overlap imdb_test & sms_train) = [9, 207]

# === Interval int_1 statistics (IMDB train & SMS train) ===
# IMDB train: selected 11408 samples in int_1.
# IMDB train label stats (label -> count): {0: 5653, 1: 5755}
# SMS train: selected 4090 samples in int_1.
# SMS train label stats (label -> count): {0: 3348, 1: 742}

# === Interval int_2 statistics (IMDB test & SMS train) ===
# IMDB test: selected 11653 samples in int_2.
# IMDB test label stats (label -> count): {0: 5708, 1: 5945}
# SMS train: selected 4981 samples in int_2.
# SMS train label stats (label -> count): {0: 4235, 1: 746}
