"""Generated by GPT-5.1, reviewed by Shiling Deng, 26/11/2025"""
import os
import json
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer


def main():
    # --------------------------------------------------------
    # 0. Load datasets
    # --------------------------------------------------------
    imdb = load_dataset("stanfordnlp/imdb")
    sms = load_dataset("ucirvine/sms_spam")  # Loaded as requested

    # # --------------------------------------------------------
    # # 1â€“4. Compute R and result from first 100 IMDB train samples
    # # --------------------------------------------------------
    # imdb_train_100 = imdb["train"].select(range(100))

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")

    # word_lengths = []
    # token_lengths = []

    # for example in imdb_train_100:
    #     sequence = example["text"]

    #     # 1. Sequence length as len(sequence.split())
    #     seq_len_words = len(sequence.split())
    #     word_lengths.append(seq_len_words)

    #     # 2. Tokenize and count tokens
    #     encoded = tokenizer(
    #         sequence,
    #         add_special_tokens=True,
    #         padding=False,
    #         truncation=False,
    #     )
    #     seq_len_tokens = len(encoded["input_ids"])
    #     token_lengths.append(seq_len_tokens)

    # word_lengths = np.array(word_lengths, dtype=float)
    # token_lengths = np.array(token_lengths, dtype=float)

    # valid_mask = token_lengths > 0
    # ratios = word_lengths[valid_mask] / token_lengths[valid_mask]

    # # 3. Average ratio R
    # R = ratios.mean()

    # # 4. Multiply R by 256
    # result = R * 256
    # threshold = int(result)

    # print(f"R (avg words/tokens): {R}")
    # print(f"result = R * 256: {result}")
    # print(f"Using threshold = int(result) = {threshold} tokens")

    # --------------------------------------------------------
    # 5. Apply threshold to each split of each dataset
    # --------------------------------------------------------
    threshold=256
    os.makedirs("filtered_data", exist_ok=True)

    # (dataset_name, loaded_dataset, text_column)
    datasets_info = [
        ("stanfordnlp/imdb", imdb, "text"),
        ("ucirvine/sms_spam", sms, "sms"),
    ]

    for dataset_name, dataset_obj, text_col in datasets_info:
        for split in dataset_obj.keys():
            ds_split = dataset_obj[split]

            print(f"\nProcessing {dataset_name} split '{split}' "
                  f"(num_rows = {len(ds_split)})")

            filtered_examples = []

            for example in ds_split:
                text = example[text_col]
                encoded = tokenizer(
                    text,
                    add_special_tokens=True,
                    padding=False,
                    truncation=False,
                )
                num_tokens = len(encoded["input_ids"])

                # Keep samples whose token length is <= threshold
                if num_tokens <= threshold:
                    filtered_examples.append(example)

            print(f"  Kept {len(filtered_examples)} examples "
                  f"(out of {len(ds_split)})")

            # Save to JSON: one file per split per dataset
            out_filename = f"{dataset_name.replace('/', '_')}_{split}.json"
            out_path = os.path.join("filtered_data", out_filename)

            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(filtered_examples, f, ensure_ascii=False, indent=2)

            print(f"  Saved filtered split to: {out_path}")


if __name__ == "__main__":
    main()

# filtered_data/stanfordnlp_imdb_train.json
# filtered_data/stanfordnlp_imdb_test.json
# filtered_data/ucirvine_sms_spam_train.json

# filtered_data/stanfordnlp_imdb_unsupervised.json


# Processing stanfordnlp/imdb split 'train' (num_rows = 25000)
#   Kept 14728 examples (out of 25000)
#   Saved filtered split to: filtered_data/stanfordnlp_imdb_train.json

# Processing stanfordnlp/imdb split 'test' (num_rows = 25000)
#   Kept 14970 examples (out of 25000)
#   Saved filtered split to: filtered_data/stanfordnlp_imdb_test.json

# Processing stanfordnlp/imdb split 'unsupervised' (num_rows = 50000)
#   Kept 29164 examples (out of 50000)
#   Saved filtered split to: filtered_data/stanfordnlp_imdb_unsupervised.json

# Processing ucirvine/sms_spam split 'train' (num_rows = 5574)
#   Kept 5573 examples (out of 5574)
#   Saved filtered split to: filtered_data/ucirvine_sms_spam_train.json
