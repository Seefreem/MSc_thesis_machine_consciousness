#!/usr/bin/env python3
from __future__ import annotations
"""
Download the datasets described in datasets/README.md and print their structure.

For the Hugging Face datasets (IMDb reviews and SMS spam), each split is treated as
an individual "file": we report the schema (features) and show the first example.
For the HaHackathon dataset, the GitHub repo is downloaded as a zip archive, extracted,
and every tabular/text data file is summarized with its columns (when applicable) and
an example row or line.
"""
"""Generated by GPT-5.1, 25/11/2025"""



import csv
import json
from itertools import islice
from pathlib import Path
from typing import Optional
from zipfile import ZipFile

import requests
from datasets import DatasetDict, load_dataset


DOWNLOAD_ROOT = Path(__file__).resolve().parent / "download_cache"
DOWNLOAD_ROOT.mkdir(parents=True, exist_ok=True)


def describe_hf_dataset(name: str, subset: Optional[str] = None) -> None:
    """Load a Hugging Face dataset and print structure per split."""
    ds_dict: DatasetDict = load_dataset(name, subset)
    print("=" * 80)
    print(f"{name} :: splits={', '.join(ds_dict.keys())}")
    for split_name, split in ds_dict.items():
        print("-" * 80)
        print(f"{name} :: split={split_name} :: num_rows={split.num_rows}")
        feature_lines = [f"{field}: {feature}" for field, feature in split.features.items()]
        print("Features:")
        for line in feature_lines:
            print(f"  - {line}")
        example = split[0] if split.num_rows else None
        print("Example:")
        if example:
            print(json.dumps(example, indent=2, ensure_ascii=False))
        else:
            print("  <empty split>")


def download_hahackathon_repo() -> Path:
    """Download and extract the HaHackathon dataset repo, returning the local path."""
    target_dir = DOWNLOAD_ROOT / "hahackathon-2021"
    if target_dir.exists():
        return target_dir

    candidate_urls = [
        "https://github.com/NLP-UMUTeam/hahackathon-2021/archive/refs/heads/main.zip",
        "https://github.com/NLP-UMUTeam/hahackathon-2021/archive/refs/heads/master.zip",
    ]
    zip_path = DOWNLOAD_ROOT / "hahackathon_repo.zip"

    for url in candidate_urls:
        try:
            print(f"Downloading HaHackathon data from {url}")
            with requests.get(url, stream=True, timeout=60) as response:
                response.raise_for_status()
                with open(zip_path, "wb") as handle:
                    for chunk in response.iter_content(chunk_size=1 << 15):
                        handle.write(chunk)
            break
        except requests.HTTPError as exc:
            print(f"Failed to download from {url}: {exc}")
    else:
        raise RuntimeError("Unable to download HaHackathon dataset from any known URL.")

    with ZipFile(zip_path) as zf:
        zf.extractall(DOWNLOAD_ROOT)
        top_level = sorted({name.split("/")[0] for name in zf.namelist() if name.strip()})

    extracted_root = None
    for candidate in top_level:
        path = DOWNLOAD_ROOT / candidate
        if path.is_dir() and candidate.startswith("hahackathon-2021"):
            extracted_root = path
            break
    if extracted_root is None:
        raise RuntimeError("Zip download did not contain expected HaHackathon folder.")

    extracted_root.rename(target_dir)
    zip_path.unlink(missing_ok=True)
    return target_dir


def describe_hahackathon_files(repo_root: Path) -> None:
    """Print structure for every relevant data file inside the HaHackathon repo."""
    interesting_suffixes = {".csv", ".tsv", ".json", ".txt"}
    files = sorted(
        path for path in repo_root.rglob("*")
        if path.is_file() and path.suffix.lower() in interesting_suffixes
    )
    print("=" * 80)
    print(f"HaHackathon :: located {len(files)} data files under {repo_root}")
    for path in files:
        rel_path = path.relative_to(repo_root)
        print("-" * 80)
        print(f"HaHackathon file: {rel_path}")
        suffix = path.suffix.lower()
        try:
            if suffix in {".csv", ".tsv"}:
                describe_tabular_file(path, delimiter="\t" if suffix == ".tsv" else ",")
            elif suffix == ".json":
                describe_json_file(path)
            else:  # .txt
                describe_text_file(path)
        except Exception as exc:  # pragma: no cover - defensive logging only
            print(f"  Failed to parse {rel_path}: {exc}")


def describe_tabular_file(path: Path, delimiter: str) -> None:
    """Print header and first data row of a CSV/TSV file."""
    with path.open(encoding="utf-8-sig", newline="") as handle:
        reader = csv.reader(handle, delimiter=delimiter)
        header = next(reader, [])
        first_row = next(reader, [])
    print(f"Columns ({len(header)}): {header}")
    if first_row:
        print(f"Example row: {first_row}")
    else:
        print("No data rows found.")


def describe_json_file(path: Path) -> None:
    """Print the top-level keys and first item of a JSON file."""
    data = json.loads(path.read_text(encoding="utf-8"))
    if isinstance(data, list) and data:
        sample = data[0]
    elif isinstance(data, dict):
        first_key = next(iter(data)) if data else None
        sample = {first_key: data[first_key]} if first_key else {}
    else:
        sample = data
    print(f"JSON type: {type(data).__name__}")
    print("Example entry:")
    print(json.dumps(sample, indent=2, ensure_ascii=False))


def describe_text_file(path: Path, num_lines: int = 3) -> None:
    """Print the first few lines of a plain text file."""
    with path.open(encoding="utf-8") as handle:
        preview = list(islice(handle, num_lines))
    print(f"Plain text preview ({len(preview)} lines):")
    for line in preview:
        print(line.rstrip("\n"))


def main() -> None:
    describe_hf_dataset("stanfordnlp/imdb")
    describe_hf_dataset("ucirvine/sms_spam")
    repo_root = download_hahackathon_repo()
    describe_hahackathon_files(repo_root)


if __name__ == "__main__":
    main()
